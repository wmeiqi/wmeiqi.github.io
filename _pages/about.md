---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# About Me

I am **Meiqi Wu <font face="Ê•∑‰Ωì">(Ê≠¶ÁæéÂ•á)</font>**, a Ph.D. student at **<a herf="https://english.ucas.ac.cn/">University of Chinese Academy of Sciences (UCAS, ‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â§ßÂ≠¶)</a>**, supervised by **<a href="https://people.ucas.ac.cn/~wqwang?language=en">Prof. Weiqiang Wang</a>**. I received my master‚Äôs degree from **<a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences (CASIA, ‰∏≠ÂõΩÁßëÂ≠¶Èô¢Ëá™Âä®ÂåñÁ†îÁ©∂ÊâÄ)</a>** and **<a herf="https://english.ucas.ac.cn/">University of Chinese Academy of Sciences (UCAS, ‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â§ßÂ≠¶)</a>**, supervised by **<a href="https://people.ucas.ac.cn/~huangkaiqi?language=en">Prof. Kaiqi Huang</a>** (IAPR Fellow, IEEE Senior Member). 


<!-- I‚Äôm currently a Ph.D. student in Computer Science advised by Prof. Weiqiang Wang, at CVMT Laboratory, School of Computer Science and Technology, University of Chinese Academy of Sciences. I‚Äôm always open to possible cooperation or visiting opportunities. If you are interested, please contact me by email. -->

My research interest includes computer vision and human-computer interaction. I‚Äôm always open to possible cooperation or visiting opportunities. If you are interested, please contact me by email.
<!--I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# üî• News
- **2024.04**:&nbsp;üì£üì£ We will present our work (**Global Instance Tracking**) at **TPAMI2023** during the [**VALSE2024**](http://www.valser.org/2024/#/) poster session (May 2024, Chongqing, China) and extend a warm invitation to colleagues interested in visual object/language tracking, evaluation methodologies, and human-computer interaction to engage in discussions with us (see our [**Poster**](https://xuchen-li.github.io/files/VALSE24Poster-364.pdf) for more information).
- **2024.04**:&nbsp;üéâüéâ One [**paper**](https://xuchen-li.github.io/#DTLLM) has been accepted by **the 3rd CVPR Workshop on Vision Datasets Understanding and DataCV Challenge** as **Oral Presentation** (CVPRW, Workshop in CCF-A Conference, Oral)!
- **2024.04**:&nbsp;üéâüéâ One first author paper has been accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT, CCF-B Journal, IF=8.4).
- **2023.09**:&nbsp;üéâüéâ One paper has been accepted by the 37th Conference on Neural Information Processing Systems (NeurIPS, CCF-A Conference, Poster).

   <!-- - *2024.04*: &nbsp;&nbsp;üéâüéâ üì£üì£We will present our work (Global Instance Tracking (GIT)) at TPAMI2023 during the VALSE2024 poster session (May 2024, Chongqing, China) and extend a warm invitation to colleagues interested in object tracking, evaluation methodologies, and human-computer interaction to engage in discussions with us (see our ü™ß Poster for more information). - *2024.04*: &nbsp;üéâüéâ One paper has been accepted by the 3rd Workshop on Vision Datasets Understanding and DataCV Challenge in CVPR 2024 (CVPRW, Workshop in CCF-A Conference, Oral). -->

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT 2024</div><img src='images/AWCV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<span class='anchor' id='AWCV'></span>
  
**Finger in Camera Speaks Everything: Unconstrained Air-Writing for Real-World**

**Meiqi Wu**(https://github.com/wmeiqi), [Kaiqi Huang](https://people.ucas.ac.cn/~huangkaiqi?language=en), [Yuanqiang Cai](https://teacher.bupt.edu.cn/caiyuanqiang/zh_CN/index.htm), [Shiyu Hu](https://huuuuusy.github.io/), [Yuzhong Zhao](https://callsys.github.io/zhaoyuzhong.github.io-main/), [Weiqiang Wang](https://people.ucas.ac.cn/~wqwang?language=en)

TCSVT 2024 (CCF-B Journal, IF=8.4): **[IEEE Transactions on Circuits and Systems for Video Technology](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76)**<br>
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2024</div><img src='images/DTLLM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<span class='anchor' id='DTLLM'></span>
  
**DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM**

[Xuchen Li](https://xuchen-li.github.io/), [Xiaokun Feng](https://github.com/XiaokunFeng), [Shiyu Hu](https://huuuuusy.github.io/), **Meiqi Wu**(https://github.com/wmeiqi), Dailing Zhang, Jing Zhang, [Kaiqi Huang](https://people.ucas.ac.cn/~huangkaiqi?language=en)

CVPRW 2024 Oral (Workshop in CCF-A Conference, Oral): **[the 3rd CVPR Workshop on Vision Datasets Understanding and DataCV Challenge](https://sites.google.com/view/vdu-cvpr24/)**<br>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/MGIT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
<span class='anchor' id='MGIT'></span>
  
**A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship**

[Shiyu Hu](https://huuuuusy.github.io/), Dailing Zhang, **Meiqi Wu**(https://github.com/wmeiqi), [Xiaokun Feng](https://github.com/XiaokunFeng), [Xuchen Li](https://xuchen-li.github.io/), [Xin Zhao](https://www.xinzhaoai.com/), [Kaiqi Huang](https://people.ucas.ac.cn/~huangkaiqi?language=en)

NeurIPS 2023 (CCF-A Conference, Poster): **[the 37th Conference on Neural Information Processing Systems](https://neurips.cc/Conferences/2023)**<br>
  [[**Paper**](https://xuchen-li.github.io/files/MGIT.pdf)]
  [[**BibTeX**](https://xuchen-li.github.io/files/MGIT.bib)]
  [[**Poster**](https://xuchen-li.github.io/files/MGIT-poster.pdf)]
  [[**Slides**](https://xuchen-li.github.io/files/MGIT-Slides.pdf)]
  [[**Platform**](http://videocube.aitestunion.com/)]
  [[**Toolkit**](https://github.com/huuuuusy/videocube-toolkit)]
  [[**Dataset**](http://videocube.aitestunion.com/downloads)]
</div>
</div>

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

# üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ü§ù Collaborators

> I am honored to collaborate with these outstanding researchers. We engage in close discussions concerning various fields such as computer vision, cognitive science, AI4Science, and human-computer interaction. If you are interested in these areas as well, please feel free to contact me.

- [**Jiahui Gao**](https://sumilergao.github.io/jiahuig.hku/), Ph.D. at the [University of Hong Kong (HKU)](https://www.hku.hk/), focusing on natural language processing, including Pre-trained Language Modeling (PLM), Automatic Machining Learning (AutoML), and Multi-Modal (vision-language) learning.
- **Yanyao Zhou**, Ph.D. student at the [University of Hong Kong (HKU)](https://www.hku.hk/), focusing on cognitive science and psychology.
- **Fangchao Liu**, Ph.D. student at the [Hong Kong University of Science and Technology (HKUST)](https://hkust.edu.hk/zh-hant), focusing on computer vision and AI4Science.
- [**Meiqi Wu**](https://github.com/wmeiqi), Ph.D. student at the [University of Chinese Academy of Sciences (UCAS)](https://www.ucas.ac.cn/), focusing on computer vision, intelligent evaluation technique, and human-computer interaction.
- **Yiping Ma**, Ph.D. student at the [East China Normal University](https://www.ecnu.edu.cn/), focusing on intelligent education technique and human-computer interaction.
- **Di Shang**, Ph.D. student at the [Institute of Automation, Chinese Academy of Sciences (CASIA)](http://www.ia.cas.cn/), focusing on computer vision, spiking neural network and few-shot learning.
- **Yaxuan Kang**, design researcher, research assistant and interaction designer at the [Institute of Automation, Chinese Academy of Sciences (CASIA)](http://www.ia.cas.cn/), focusing on human-computer interaction.
- **Jing Zhang**, research assistant at the [Institute of Automation, Chinese Academy of Sciences (CASIA)](http://www.ia.cas.cn/), focusing on computer vision and AI4Science.
- [**Yipei Wang**](https://github.com/updateforever), M.S. and incoming Ph.D. student at the [Southeast University](https://www.seu.edu.cn/), focusing on visual object tracking and recommendation system.
- [**Xiaokun Feng**](https://github.com/XiaokunFeng), Ph.D. student at the [Institute of Automation, Chinese Academy of Sciences (CASIA)](http://www.ia.cas.cn/), focusing on visual object tracking, visual language tracking and AI4Science.
- **Dailing Zhang**, Ph.D. student at the [Institute of Automation, Chinese Academy of Sciences (CASIA)](http://www.ia.cas.cn/), focusing on visual object tracking, visual language tracking and AI4Science.
- [**Xuchen Li**](https://xuchen-li.github.io/), B.E. student at [Beijing University of Posts and Telecommunications (BUPT)](https://xuchen-li.github.io/) and incoming Ph.D. student at the [Institute of Automation, Chinese Academy of Sciences (CASIA)](http://www.ia.cas.cn/), focusing on visual object tracking, visual language tracking and AI4Science.

# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
